% Section 3: Results and Analysis
% Present results as formatted tables and figures.
% Should have at least one table or figure for each of:
% - 2.3 (Naive Bayes)
% - 2.4 (Topic Modeling)
% - 2.5 (Experimentation)
% Include description of main takeaways for each table/figure.

\section{Results and Analysis}

\subsection{Naive Bayes Analysis}

The Naive Bayes log-likelihood ratio analysis identified words that are most strongly associated with each category. Table~\ref{tab:naive_bayes} shows the top 10 words per category sorted by their LLR scores.

\begin{table}[H]
\centering
\caption{Top 10 words per category by log-likelihood ratio}
\label{tab:naive_bayes}
\begin{tabular}{lll}
\toprule
Category & Rank & Word (LLR) \\
\midrule
Error & 1 & \texttt{authentication} (5.9178) \\
Error & 2 & \texttt{uid=0} (5.8100) \\
Error & 3 & \texttt{euid=0} (5.8100) \\
Error & 4 & \texttt{failure;} (5.8100) \\
Error & 5 & \texttt{ruser=} (5.8100) \\
Error & 6 & \texttt{logname=} (5.8100) \\
Error & 7 & \texttt{tty=nodevssh} (5.8100) \\
Error & 8 & \texttt{user=root} (5.2068) \\
Error & 9 & \texttt{jul} (4.2153) \\
Error & 10 & \texttt{failed} (3.7789) \\
Normal & 1 & \texttt{succeeded} (4.2168) \\
Normal & 2 & \texttt{device} (4.1987) \\
Normal & 3 & \texttt{node} (4.0626) \\
Normal & 4 & \texttt{11:31:47} (3.9756) \\
Normal & 5 & \texttt{startup} (3.8803) \\
Normal & 6 & \texttt{kernel:} (3.7340) \\
Normal & 7 & \texttt{named[2275]:} (3.6254) \\
Normal & 8 & \texttt{creating} (3.5926) \\
Normal & 9 & \texttt{driver} (3.5926) \\
Normal & 10 & \texttt{06:06:24} (3.5587) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Main takeaways}: The error category is strongly characterized by authentication-related terms (``authentication'', ``uid=0'', ``euid=0'', ``failure;'', ``ruser='', ``logname='', ``tty=nodevssh'') and failure indicators (``failed''). These terms reflect SSH authentication failures and permission issues. The normal category is characterized by successful operations (``succeeded'', ``startup'', ``creating'') and system components (``device'', ``node'', ``kernel:'', ``driver'', ``named[2275]:''), indicating routine system operations and service initialization.

\subsection{Topic Modeling}

Latent Dirichlet Allocation with 10 topics was applied to the corpus. For each topic, the top 25 terms were extracted and manually assigned topic labels based on the most prominent terms. Tables~\ref{tab:topics1}--\ref{tab:topics3} show the top 10 terms for each topic (selected from the top 25 for readability), organized into three compact tables.

\begin{table}[H]
\centering
\caption{Top terms per topic - Part 1 (Topics 1-4)}
\label{tab:topics1}
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{p{2.5cm}p{11cm}}
\toprule
Topic & Top 10 Terms (Probability) \\
\midrule
System Startup & \texttt{kernel:} (0.0760), \texttt{9} (0.0609), \texttt{10} (0.0368), \texttt{06:06:20} (0.0185), \texttt{succeeded} (0.0184), \texttt{11:31:44} (0.0180), \texttt{startup} (0.0170), \texttt{11:31:46} (0.0153), \texttt{bios} (0.0138), \texttt{hda:} (0.0128) \\
\midrule
Kernel Operations & \texttt{kernel:} (0.0500), \texttt{9} (0.0381), \texttt{10} (0.0256), \texttt{rhost=61.153.202.254} (0.0185), \texttt{10:18:09} (0.0185), \texttt{version} (0.0183), \texttt{1} (0.0143), \texttt{irq} (0.0134), \texttt{linux} (0.0123), \texttt{pci} (0.0112) \\
\midrule
Device Detection & \texttt{kernel:} (0.0568), \texttt{10} (0.0464), \texttt{9} (0.0351), \texttt{11:31:45} (0.0187), \texttt{06:06:21} (0.0185), \texttt{found} (0.0163), \texttt{check} (0.0159), \texttt{machine} (0.0159), \texttt{enabled} (0.0159), \texttt{11:31:47} (0.0151) \\
\midrule
Process Management & \texttt{9} (0.0836), \texttt{device} (0.0573), \texttt{node} (0.0496), \texttt{alert} (0.0332), \texttt{logrotate:} (0.0332), \texttt{abnormally} (0.0332), \texttt{[1]} (0.0332), \texttt{exited} (0.0332), \texttt{creating} (0.0320), \texttt{kernel:} (0.0266) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Top terms per topic - Part 2 (Topics 5-7)}
\label{tab:topics2}
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{p{2.5cm}p{11cm}}
\toprule
Topic & Top 10 Terms (Probability) \\
\midrule
Authentication Failures & \texttt{authentication} (0.0899), \texttt{tty=nodevssh} (0.0899), \texttt{logname=} (0.0899), \texttt{uid=0} (0.0899), \texttt{euid=0} (0.0899), \texttt{failure;} (0.0899), \texttt{ruser=} (0.0899), \texttt{user=root} (0.0570), \texttt{22} (0.0196), \texttt{28} (0.0148) \\
\midrule
Network Services & \texttt{9} (0.0943), \texttt{kernel:} (0.0887), \texttt{10} (0.0426), \texttt{named[2275]:} (0.0201), \texttt{11:31:45} (0.0173), \texttt{ipv4} (0.0168), \texttt{listening} (0.0168), \texttt{interface} (0.0168), \texttt{06:06:21} (0.0155), \texttt{06:06:24} (0.0141) \\
\midrule
SSH Authentication & \texttt{euid=0} (0.0880), \texttt{uid=0} (0.0880), \texttt{tty=nodevssh} (0.0879), \texttt{logname=} (0.0879), \texttt{ruser=} (0.0879), \texttt{failure;} (0.0879), \texttt{authentication} (0.0879), \texttt{user=root} (0.0555), \texttt{jul} (0.0393), \texttt{13} (0.0173) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Top terms per topic - Part 3 (Topics 8-10)}
\label{tab:topics3}
\footnotesize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{p{2.5cm}p{11cm}}
\toprule
Topic & Top 10 Terms (Probability) \\
\midrule
Kernel Messages & \texttt{kernel:} (0.1053), \texttt{10} (0.0656), \texttt{9} (0.0618), \texttt{11:31:44} (0.0328), \texttt{06:06:20} (0.0267), \texttt{11:31:47} (0.0197), \texttt{bios-e820:} (0.0125), \texttt{-} (0.0125), \texttt{ext3-fs:} (0.0100), \texttt{driver} (0.0094) \\
\midrule
Kerberos Authentication & \texttt{failed} (0.0814), \texttt{30} (0.0752), \texttt{authentication} (0.0721), \texttt{20:53:06} (0.0438), \texttt{163.27.187.39} (0.0362), \texttt{kerberos} (0.0360), \texttt{succeeded} (0.0308), \texttt{startup} (0.0287), \texttt{20:53:04} (0.0284), \texttt{connection} (0.0250) \\
\midrule
Authentication Errors & \texttt{euid=0} (0.0836), \texttt{uid=0} (0.0836), \texttt{failure;} (0.0835), \texttt{tty=nodevssh} (0.0835), \texttt{authentication} (0.0835), \texttt{logname=} (0.0835), \texttt{ruser=} (0.0835), \texttt{15} (0.0348), \texttt{rhost=218.188.2.4} (0.0279), \texttt{12} (0.0242) \\
\bottomrule
\end{tabular}
\end{table}

\clearpage
Table~\ref{tab:category_topics} shows the top 5 topics for each category, representing the average topic distribution across documents in that category.

\begin{table}[H]
\centering
\caption{Top 5 topics per category}
\label{tab:category_topics}
\begin{tabular}{lll}
\toprule
Category & Rank & Topic (Probability) \\
\midrule
Error & 1 & Topic 6 (0.2956) \\
Error & 2 & Topic 4 (0.2851) \\
Error & 3 & Topic 9 (0.1878) \\
Error & 4 & Topic 8 (0.1055) \\
Error & 5 & Topic 3 (0.0624) \\
Normal & 1 & Topic 5 (0.3466) \\
Normal & 2 & Topic 7 (0.1728) \\
Normal & 3 & Topic 3 (0.1711) \\
Normal & 4 & Topic 0 (0.0946) \\
Normal & 5 & Topic 1 (0.0702) \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Main takeaways}: The error category is dominated by authentication-related topics (Topics 6, 4, and 9: SSH Authentication, Authentication Failures, and Authentication Errors), which together account for 76.8\% of the topic distribution. The normal category is dominated by Topic 5 (Network Services) at 34.7\%, followed by Topic 7 (Kernel Messages) and Topic 3 (Process Management), reflecting routine system operations. The topic modeling successfully separated authentication failures from successful system operations, demonstrating that LDA can identify meaningful thematic patterns in log data.

An interactive visualization of the topic model is available in the file \texttt{lda\_visualization.html} (located in \texttt{output/topic\_modeling/}), which allows exploration of topic relationships and term distributions.

\clearpage
\subsection{Experimentation}

To understand the effects of different preprocessing configurations, I tested five normalization approaches with topic modeling, measuring coherence scores as an indicator of topic quality. Table~\ref{tab:experiments} shows the results.

\begin{table}[H]
\centering
\caption{Comparison of preprocessing configurations}
\label{tab:experiments}
\begin{tabular}{lcc}
\toprule
Configuration & Vocab Size & Coherence \\
\midrule
raw & 1009 & 0.4319 \\
lowercase & 970 & 0.4281 \\
lowercase\_stopwords & 934 & 0.4559 \\
lowercase\_stopwords\_stem & 913 & 0.3798 \\
lowercase\_stopwords\_lemmatize & 929 & 0.4701 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Main takeaways}: The configuration with lowercase, stopword removal, and lemmatization achieved the highest coherence score (0.4701), indicating that this preprocessing combination produces the most interpretable topics. Lowercasing and stopword removal reduced vocabulary size from 1009 to 934 tokens while improving coherence from 0.4319 to 0.4559. Interestingly, lemmatization (coherence 0.4701) outperformed stemming (coherence 0.3798), suggesting that preserving valid dictionary forms is more beneficial for log text than aggressive stemming. The raw configuration had the largest vocabulary but lower coherence, indicating that preprocessing improves topic quality by reducing noise and normalizing word forms.
