% Section 2: Methodology
% Describe the steps performed and what informed your decisions.
% Include details of preprocessing steps (lowercasing, stemming, etc.)
% Describe the kind of analysis performed.
% Mention which packages/libraries were used for steps not implemented yourself.

\section{Methodology}

\subsection{Preprocessing}

The preprocessing pipeline reuses and extends functionality from Homework 1. For the main analysis, I applied lowercase conversion and stopword removal, as these preprocessing steps improved topic coherence scores in preliminary experiments. The following normalization options were available:

\begin{itemize}
    \item \textbf{Lowercasing}: Converts all tokens to lowercase to reduce vocabulary size. This was applied to all analyses.
    \item \textbf{Stopword Removal}: Removes common English stopwords using the NLTK stopword list. This was applied to all analyses.
    \item \textbf{Stemming}: Applies Porter stemming to reduce words to their root forms. Tested in experiments but not used in main analysis.
    \item \textbf{Lemmatization}: Applies WordNet lemmatization to normalize words to their dictionary form (mutually exclusive with stemming). Tested in experiments and found to improve topic coherence.
    \item \textbf{Digit Removal}: Removes tokens that consist only of digits (custom option from HW-1). Not applied in this assignment.
\end{itemize}

Tokenization is performed using manual whitespace splitting, as in Homework 1. All file operations use UTF-8 encoding with error replacement for robustness. The main analysis used lowercase conversion and stopword removal, which reduced vocabulary size from 1009 (raw) to 934 tokens while improving topic coherence from 0.4319 to 0.4559.

\subsection{Bag-of-Words Conversion}

Documents were converted to bag-of-words format using scipy sparse matrices for memory efficiency. The main analysis used count-based representation (raw token counts per document). Three representation types were supported and tested in experiments:
\begin{itemize}
    \item \textbf{Count}: Raw token counts per document (used in main analysis)
    \item \textbf{Binary}: Binary indicators (1 if token appears, 0 otherwise)
    \item \textbf{TF-IDF}: Term frequency-inverse document frequency transformation
\end{itemize}

\subsection{Naive Bayes Analysis}

Naive Bayes probabilities were computed for each word and category:
\begin{itemize}
    \item $P(w|c)$: Probability of word $w$ given category $c$ (with add-one smoothing)
    \item $P(w|C_o)$: Probability of word $w$ given other categories (equation 2 from assignment)
    \item Log-likelihood ratio: $\mathrm{LLR}(w,c) = \log(P(w|c)) - \log(P(w|C_o))$
\end{itemize}

The top 10 words per category were extracted based on LLR scores.

\subsection{Topic Modeling}

Latent Dirichlet Allocation (LDA) was performed using the \texttt{gensim} library. The number of topics was chosen based on coherence optimization or manual inspection. For each topic:
\begin{itemize}
    \item Top 25 terms with probabilities were extracted
    \item Topic distributions per document were computed
    \item Average topic distributions per category were calculated
    \item Top 3-5 topics per category were identified
\end{itemize}

Visualizations were generated using \texttt{pyLDAvis}.

\subsection{Libraries Used}

\begin{itemize}
    \item \texttt{gensim}: LDA topic modeling
    \item \texttt{pyLDAvis}: Topic visualization
    \item \texttt{scipy}: Sparse matrix operations
    \item \texttt{scikit-learn}: TF-IDF transformation
    \item \texttt{nltk}: Preprocessing (stopwords, stemming, lemmatization)
    \item \texttt{numpy}, \texttt{pandas}: Data manipulation
\end{itemize}
